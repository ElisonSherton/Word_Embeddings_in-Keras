{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Word Embedding in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll demonstrate how to use the Embedding layer in keras for doing text classification. The task at hand is to build a classifier to distinguish good reviews from bad ones in amazon reviews of miscellaneous products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the data is: (1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here in the US unless I go by a converter.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 Text  \\\n",
       "0  So there is no way for me to plug it in here in the US unless I go by a converter.   \n",
       "1  Good case, Excellent value.                                                          \n",
       "2  Great for the jawbone.                                                               \n",
       "3  Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!      \n",
       "4  The mic is great.                                                                    \n",
       "\n",
       "   Reviews  \n",
       "0  0        \n",
       "1  1        \n",
       "2  1        \n",
       "3  0        \n",
       "4  1        "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table('amazon_cells_labelled.txt', header = None)\n",
    "data.columns = ['Text', 'Reviews']\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(f\"Size of the data is: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y = \"Reviews\", data = data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.Text; y = data.Reviews\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.05, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Tokenization\n",
    "We need to tokenize the text into integer indices and express the sentences or reviews as a set of integers.\n",
    "\n",
    "**Tokenization**:\n",
    "\n",
    "Keras' `Tokenizer()` helps achieve this task.\n",
    "\n",
    "Tokenizer's built in `fit_on_texts` method will take in the documents as input and assign an index to each and every unique word in the document.\n",
    "\n",
    "We could check this using the `word_index` attribute in the Tokenizer.\n",
    "\n",
    "## Encoding\n",
    "\n",
    "We need to encode the sentences using these indices obtained as a result of the tokenization above. The `Tokenizer` object created in the previous step has a method `text_to_sequences` which helps us do this task easily.\n",
    "\n",
    "## Uniform Length Padding\n",
    "\n",
    "Keras prefers it's input documents to be all of the same length. Hence we'll have to find out the length of the longest review and make every other review of the same length by using `pad_sequences` method in keras which will basically append 0s post the shorter reviews to make their length equal that of the review with maximum tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1838 unique words in our vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "Tk = Tokenizer()\n",
    "Tk.fit_on_texts(X_train)\n",
    "\n",
    "# Watch how big is the vocabulary\n",
    "print(f\"There are {len(Tk.word_index)} unique words in our vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text vocabulary is of size 1839.\n"
     ]
    }
   ],
   "source": [
    "# Define the vocabulary size accordingly\n",
    "vocab_size = len(Tk.word_index) + 1\n",
    "print(f\"Our text vocabulary is of size {vocab_size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all the sentences in our data to convert them into integer index representations.\n",
    "encoded_train = Tk.texts_to_sequences(X_train)\n",
    "encoded_test = Tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181, 9]\n",
      "[2, 257, 36, 7, 54, 114, 1, 534, 5, 781]\n",
      "[535, 93, 2, 324]\n",
      "[2, 83, 100, 55, 107, 14, 7, 24]\n",
      "[1, 122, 398, 1, 9, 325, 17, 3, 123, 56, 108, 1, 168, 326, 226, 3, 1, 115, 11, 37, 4, 782, 536, 783]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The integer index coded representation for each review is as follows\n",
    "[print(i) for i in encoded_train[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest review has 30 number of words present in it.\n"
     ]
    }
   ],
   "source": [
    "# Let's find out the length of the biggest review for padding purposes\n",
    "max_length_train = max([len(i) for i in encoded_train])\n",
    "max_length_test = max([len(i) for i in encoded_test])\n",
    "\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "print(f\"The biggest review has {max_length} number of words present in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[  2 257  36   7  54 114   1 534   5 781   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[535  93   2 324   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[  2  83 100  55 107  14   7  24   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[  1 122 398   1   9 325  17   3 123  56 108   1 168 326 226   3   1 115\n",
      "  11  37   4 782 536 783   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the documents to the maximum length for keras to accept input while building a model\n",
    "padded_train = pad_sequences(encoded_train, maxlen = max_length, padding = 'post')\n",
    "padded_test = pad_sequences(encoded_test, maxlen = max_length, padding = 'post')\n",
    "\n",
    "# Top 5 results in padded train sequence. \n",
    "# We can see that each and every review has been padded with 0s at it's \n",
    "# end to bring the lengths of all the datapoints on the same baseline\n",
    "[print(i) for i in padded_train[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data has a vocabulary size of 1839.\n",
    "\n",
    "Each input has 30 words or 30 tokens associated with it.\n",
    "\n",
    "Let us choose an embedding dimension of 50 in order to keep things simple.\n",
    "\n",
    "Every word will return a 50 dimensional vector as it's embedded output.\n",
    "\n",
    "There are 30 such words per datapoint which means we'll be getting 30 * 50 = 1500 output activations which need to be flattened to bring them into one single layer which can be fed to the next FC layer for doing binary classification.\n",
    "\n",
    "Let's construct this model using the `Sequential` API in keras\n",
    "\n",
    "**NOTE: Embedding layer parameters**:\n",
    "\n",
    "- input_dim - It is the size of our vocabulary (1839 in our case).\n",
    "- input_length - Length of input sequences (30 in our case).\n",
    "- output_dimension - Dimension of the dense embedding (50 which we have arbit selected).\n",
    "\n",
    "[Refer this link](https://keras.io/layers/embeddings/) for details about several other parameters which we won't be using here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 50 # It is the neurons in the embedding layer.\n",
    "model = Sequential([\n",
    "    Embedding(input_dim = vocab_size, input_length = max_length, output_dim = d),\n",
    "    Flatten(),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 50)            91950     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1501      \n",
      "=================================================================\n",
      "Total params: 93,451\n",
      "Trainable params: 93,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `Embedding` layer has to learn 50 * 1839 = 91950 weights to uniquely represent each word as a vector of dimension 50. \n",
    "\n",
    "Since each sentence/review is only 30 words long and each word is described by 50 neuron activations, we get the training parameters of the NN architecture in the first layer to be 30 * 50 = 1500 which are flattened with `Flatten` layer.\n",
    "\n",
    "Eventually we have a `Dense` layer which does the job of classifying the reviews into 1s(positive) or 0s(negative)\n",
    "\n",
    "Let's now compile the model and use sgd with learning rate 0.01 to optimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = SGD(lr = learning_rate), loss =  'binary_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit & Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 760 samples, validate on 190 samples\n",
      "Epoch 1/200\n",
      "760/760 [==============================] - 0s 346us/step - loss: 0.6924 - accuracy: 0.5276 - val_loss: 0.6945 - val_accuracy: 0.4737\n",
      "Epoch 2/200\n",
      "760/760 [==============================] - 0s 92us/step - loss: 0.6918 - accuracy: 0.5395 - val_loss: 0.6961 - val_accuracy: 0.4737\n",
      "Epoch 3/200\n",
      "760/760 [==============================] - 0s 97us/step - loss: 0.6910 - accuracy: 0.5303 - val_loss: 0.6950 - val_accuracy: 0.4789\n",
      "Epoch 4/200\n",
      "760/760 [==============================] - 0s 101us/step - loss: 0.6900 - accuracy: 0.5553 - val_loss: 0.6951 - val_accuracy: 0.4737\n",
      "Epoch 5/200\n",
      "760/760 [==============================] - 0s 138us/step - loss: 0.6894 - accuracy: 0.5316 - val_loss: 0.6938 - val_accuracy: 0.4895\n",
      "Epoch 6/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.6884 - accuracy: 0.5408 - val_loss: 0.6919 - val_accuracy: 0.5421\n",
      "Epoch 7/200\n",
      "760/760 [==============================] - 0s 124us/step - loss: 0.6876 - accuracy: 0.6013 - val_loss: 0.6940 - val_accuracy: 0.4789\n",
      "Epoch 8/200\n",
      "760/760 [==============================] - 0s 124us/step - loss: 0.6869 - accuracy: 0.5632 - val_loss: 0.6928 - val_accuracy: 0.5105\n",
      "Epoch 9/200\n",
      "760/760 [==============================] - 0s 103us/step - loss: 0.6863 - accuracy: 0.5974 - val_loss: 0.6933 - val_accuracy: 0.4789\n",
      "Epoch 10/200\n",
      "760/760 [==============================] - 0s 118us/step - loss: 0.6856 - accuracy: 0.5671 - val_loss: 0.6922 - val_accuracy: 0.5211\n",
      "Epoch 11/200\n",
      "760/760 [==============================] - 0s 114us/step - loss: 0.6843 - accuracy: 0.5974 - val_loss: 0.6916 - val_accuracy: 0.5263\n",
      "Epoch 12/200\n",
      "760/760 [==============================] - 0s 101us/step - loss: 0.6839 - accuracy: 0.5987 - val_loss: 0.6922 - val_accuracy: 0.5316\n",
      "Epoch 13/200\n",
      "760/760 [==============================] - 0s 100us/step - loss: 0.6829 - accuracy: 0.6132 - val_loss: 0.6922 - val_accuracy: 0.5421\n",
      "Epoch 14/200\n",
      "760/760 [==============================] - 0s 104us/step - loss: 0.6821 - accuracy: 0.6039 - val_loss: 0.6910 - val_accuracy: 0.5158\n",
      "Epoch 15/200\n",
      "760/760 [==============================] - 0s 116us/step - loss: 0.6809 - accuracy: 0.6145 - val_loss: 0.6928 - val_accuracy: 0.5263\n",
      "Epoch 16/200\n",
      "760/760 [==============================] - 0s 110us/step - loss: 0.6802 - accuracy: 0.6171 - val_loss: 0.6911 - val_accuracy: 0.5211\n",
      "Epoch 17/200\n",
      "760/760 [==============================] - 0s 117us/step - loss: 0.6792 - accuracy: 0.5961 - val_loss: 0.6884 - val_accuracy: 0.5368\n",
      "Epoch 18/200\n",
      "760/760 [==============================] - 0s 120us/step - loss: 0.6783 - accuracy: 0.6092 - val_loss: 0.6868 - val_accuracy: 0.5579\n",
      "Epoch 19/200\n",
      "760/760 [==============================] - 0s 125us/step - loss: 0.6772 - accuracy: 0.6158 - val_loss: 0.6893 - val_accuracy: 0.5368\n",
      "Epoch 20/200\n",
      "760/760 [==============================] - 0s 120us/step - loss: 0.6760 - accuracy: 0.6342 - val_loss: 0.6878 - val_accuracy: 0.5316\n",
      "Epoch 21/200\n",
      "760/760 [==============================] - 0s 104us/step - loss: 0.6750 - accuracy: 0.6171 - val_loss: 0.6889 - val_accuracy: 0.5421\n",
      "Epoch 22/200\n",
      "760/760 [==============================] - 0s 118us/step - loss: 0.6739 - accuracy: 0.6145 - val_loss: 0.6887 - val_accuracy: 0.5474\n",
      "Epoch 23/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.6724 - accuracy: 0.6408 - val_loss: 0.6869 - val_accuracy: 0.5421\n",
      "Epoch 24/200\n",
      "760/760 [==============================] - 0s 99us/step - loss: 0.6709 - accuracy: 0.6329 - val_loss: 0.6876 - val_accuracy: 0.5526\n",
      "Epoch 25/200\n",
      "760/760 [==============================] - 0s 108us/step - loss: 0.6695 - accuracy: 0.6526 - val_loss: 0.6875 - val_accuracy: 0.5684\n",
      "Epoch 26/200\n",
      "760/760 [==============================] - 0s 109us/step - loss: 0.6684 - accuracy: 0.6645 - val_loss: 0.6846 - val_accuracy: 0.5421\n",
      "Epoch 27/200\n",
      "760/760 [==============================] - 0s 124us/step - loss: 0.6667 - accuracy: 0.6658 - val_loss: 0.6808 - val_accuracy: 0.5789\n",
      "Epoch 28/200\n",
      "760/760 [==============================] - 0s 114us/step - loss: 0.6657 - accuracy: 0.6513 - val_loss: 0.6833 - val_accuracy: 0.5421\n",
      "Epoch 29/200\n",
      "760/760 [==============================] - 0s 118us/step - loss: 0.6636 - accuracy: 0.6526 - val_loss: 0.6838 - val_accuracy: 0.5632\n",
      "Epoch 30/200\n",
      "760/760 [==============================] - 0s 124us/step - loss: 0.6619 - accuracy: 0.6566 - val_loss: 0.6821 - val_accuracy: 0.5421\n",
      "Epoch 31/200\n",
      "760/760 [==============================] - 0s 117us/step - loss: 0.6599 - accuracy: 0.6645 - val_loss: 0.6827 - val_accuracy: 0.5789\n",
      "Epoch 32/200\n",
      "760/760 [==============================] - 0s 95us/step - loss: 0.6579 - accuracy: 0.6711 - val_loss: 0.6837 - val_accuracy: 0.6000\n",
      "Epoch 33/200\n",
      "760/760 [==============================] - 0s 121us/step - loss: 0.6563 - accuracy: 0.6908 - val_loss: 0.6791 - val_accuracy: 0.5526\n",
      "Epoch 34/200\n",
      "760/760 [==============================] - 0s 125us/step - loss: 0.6540 - accuracy: 0.6882 - val_loss: 0.6776 - val_accuracy: 0.5474\n",
      "Epoch 35/200\n",
      "760/760 [==============================] - 0s 105us/step - loss: 0.6516 - accuracy: 0.6947 - val_loss: 0.6780 - val_accuracy: 0.5789\n",
      "Epoch 36/200\n",
      "760/760 [==============================] - 0s 116us/step - loss: 0.6495 - accuracy: 0.7026 - val_loss: 0.6768 - val_accuracy: 0.5789\n",
      "Epoch 37/200\n",
      "760/760 [==============================] - 0s 117us/step - loss: 0.6472 - accuracy: 0.7092 - val_loss: 0.6732 - val_accuracy: 0.5632\n",
      "Epoch 38/200\n",
      "760/760 [==============================] - 0s 128us/step - loss: 0.6447 - accuracy: 0.7145 - val_loss: 0.6707 - val_accuracy: 0.5632\n",
      "Epoch 39/200\n",
      "760/760 [==============================] - 0s 116us/step - loss: 0.6420 - accuracy: 0.7092 - val_loss: 0.6731 - val_accuracy: 0.5895\n",
      "Epoch 40/200\n",
      "760/760 [==============================] - 0s 128us/step - loss: 0.6392 - accuracy: 0.7329 - val_loss: 0.6705 - val_accuracy: 0.5947\n",
      "Epoch 41/200\n",
      "760/760 [==============================] - 0s 129us/step - loss: 0.6364 - accuracy: 0.7289 - val_loss: 0.6689 - val_accuracy: 0.5895\n",
      "Epoch 42/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.6334 - accuracy: 0.7276 - val_loss: 0.6680 - val_accuracy: 0.6000\n",
      "Epoch 43/200\n",
      "760/760 [==============================] - 0s 113us/step - loss: 0.6298 - accuracy: 0.7526 - val_loss: 0.6660 - val_accuracy: 0.5895\n",
      "Epoch 44/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.6262 - accuracy: 0.7474 - val_loss: 0.6702 - val_accuracy: 0.6105\n",
      "Epoch 45/200\n",
      "760/760 [==============================] - 0s 87us/step - loss: 0.6235 - accuracy: 0.7553 - val_loss: 0.6602 - val_accuracy: 0.5737\n",
      "Epoch 46/200\n",
      "760/760 [==============================] - 0s 97us/step - loss: 0.6194 - accuracy: 0.7645 - val_loss: 0.6588 - val_accuracy: 0.5895\n",
      "Epoch 47/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.6158 - accuracy: 0.7684 - val_loss: 0.6575 - val_accuracy: 0.5895\n",
      "Epoch 48/200\n",
      "760/760 [==============================] - 0s 96us/step - loss: 0.6122 - accuracy: 0.7671 - val_loss: 0.6546 - val_accuracy: 0.5947\n",
      "Epoch 49/200\n",
      "760/760 [==============================] - 0s 110us/step - loss: 0.6083 - accuracy: 0.7618 - val_loss: 0.6540 - val_accuracy: 0.6000\n",
      "Epoch 50/200\n",
      "760/760 [==============================] - 0s 100us/step - loss: 0.6040 - accuracy: 0.7829 - val_loss: 0.6496 - val_accuracy: 0.6000\n",
      "Epoch 51/200\n",
      "760/760 [==============================] - 0s 110us/step - loss: 0.5989 - accuracy: 0.7842 - val_loss: 0.6432 - val_accuracy: 0.6105\n",
      "Epoch 52/200\n",
      "760/760 [==============================] - 0s 91us/step - loss: 0.5959 - accuracy: 0.7934 - val_loss: 0.6438 - val_accuracy: 0.6211\n",
      "Epoch 53/200\n",
      "760/760 [==============================] - 0s 107us/step - loss: 0.5912 - accuracy: 0.7921 - val_loss: 0.6456 - val_accuracy: 0.6211\n",
      "Epoch 54/200\n",
      "760/760 [==============================] - 0s 108us/step - loss: 0.5861 - accuracy: 0.8105 - val_loss: 0.6440 - val_accuracy: 0.6158\n",
      "Epoch 55/200\n",
      "760/760 [==============================] - 0s 105us/step - loss: 0.5813 - accuracy: 0.8079 - val_loss: 0.6375 - val_accuracy: 0.6263\n",
      "Epoch 56/200\n",
      "760/760 [==============================] - 0s 105us/step - loss: 0.5767 - accuracy: 0.8145 - val_loss: 0.6366 - val_accuracy: 0.6211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "760/760 [==============================] - 0s 89us/step - loss: 0.5715 - accuracy: 0.8132 - val_loss: 0.6354 - val_accuracy: 0.6211\n",
      "Epoch 58/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.5668 - accuracy: 0.8211 - val_loss: 0.6291 - val_accuracy: 0.6316\n",
      "Epoch 59/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.5613 - accuracy: 0.8197 - val_loss: 0.6308 - val_accuracy: 0.6053\n",
      "Epoch 60/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.5564 - accuracy: 0.8158 - val_loss: 0.6217 - val_accuracy: 0.6632\n",
      "Epoch 61/200\n",
      "760/760 [==============================] - 0s 72us/step - loss: 0.5508 - accuracy: 0.8237 - val_loss: 0.6213 - val_accuracy: 0.6474\n",
      "Epoch 62/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.5458 - accuracy: 0.8316 - val_loss: 0.6168 - val_accuracy: 0.6632\n",
      "Epoch 63/200\n",
      "760/760 [==============================] - 0s 68us/step - loss: 0.5401 - accuracy: 0.8316 - val_loss: 0.6124 - val_accuracy: 0.6579\n",
      "Epoch 64/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.5344 - accuracy: 0.8395 - val_loss: 0.6098 - val_accuracy: 0.6684\n",
      "Epoch 65/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.5288 - accuracy: 0.8395 - val_loss: 0.6102 - val_accuracy: 0.6579\n",
      "Epoch 66/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.5230 - accuracy: 0.8342 - val_loss: 0.6051 - val_accuracy: 0.6632\n",
      "Epoch 67/200\n",
      "760/760 [==============================] - 0s 87us/step - loss: 0.5173 - accuracy: 0.8355 - val_loss: 0.5977 - val_accuracy: 0.6737\n",
      "Epoch 68/200\n",
      "760/760 [==============================] - 0s 95us/step - loss: 0.5118 - accuracy: 0.8447 - val_loss: 0.5979 - val_accuracy: 0.6632\n",
      "Epoch 69/200\n",
      "760/760 [==============================] - 0s 92us/step - loss: 0.5059 - accuracy: 0.8461 - val_loss: 0.5917 - val_accuracy: 0.6842\n",
      "Epoch 70/200\n",
      "760/760 [==============================] - 0s 85us/step - loss: 0.5002 - accuracy: 0.8500 - val_loss: 0.5924 - val_accuracy: 0.6684\n",
      "Epoch 71/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.4946 - accuracy: 0.8539 - val_loss: 0.5918 - val_accuracy: 0.6526\n",
      "Epoch 72/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.4891 - accuracy: 0.8592 - val_loss: 0.5855 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.4826 - accuracy: 0.8605 - val_loss: 0.5846 - val_accuracy: 0.6526\n",
      "Epoch 74/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.4770 - accuracy: 0.8658 - val_loss: 0.5767 - val_accuracy: 0.6842\n",
      "Epoch 75/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.4717 - accuracy: 0.8605 - val_loss: 0.5761 - val_accuracy: 0.6842\n",
      "Epoch 76/200\n",
      "760/760 [==============================] - 0s 68us/step - loss: 0.4658 - accuracy: 0.8711 - val_loss: 0.5731 - val_accuracy: 0.6842\n",
      "Epoch 77/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.4601 - accuracy: 0.8632 - val_loss: 0.5676 - val_accuracy: 0.7105\n",
      "Epoch 78/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.4557 - accuracy: 0.8684 - val_loss: 0.5653 - val_accuracy: 0.7105\n",
      "Epoch 79/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.4492 - accuracy: 0.8750 - val_loss: 0.5657 - val_accuracy: 0.6895\n",
      "Epoch 80/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.4438 - accuracy: 0.8737 - val_loss: 0.5624 - val_accuracy: 0.6842\n",
      "Epoch 81/200\n",
      "760/760 [==============================] - 0s 99us/step - loss: 0.4383 - accuracy: 0.8803 - val_loss: 0.5569 - val_accuracy: 0.7211\n",
      "Epoch 82/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.4338 - accuracy: 0.8855 - val_loss: 0.5547 - val_accuracy: 0.7211\n",
      "Epoch 83/200\n",
      "760/760 [==============================] - 0s 91us/step - loss: 0.4275 - accuracy: 0.8908 - val_loss: 0.5590 - val_accuracy: 0.6737\n",
      "Epoch 84/200\n",
      "760/760 [==============================] - 0s 80us/step - loss: 0.4222 - accuracy: 0.8868 - val_loss: 0.5533 - val_accuracy: 0.6895\n",
      "Epoch 85/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.4172 - accuracy: 0.8974 - val_loss: 0.5506 - val_accuracy: 0.7053\n",
      "Epoch 86/200\n",
      "760/760 [==============================] - 0s 87us/step - loss: 0.4119 - accuracy: 0.8908 - val_loss: 0.5481 - val_accuracy: 0.7105\n",
      "Epoch 87/200\n",
      "760/760 [==============================] - 0s 96us/step - loss: 0.4066 - accuracy: 0.8921 - val_loss: 0.5487 - val_accuracy: 0.6789\n",
      "Epoch 88/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.4016 - accuracy: 0.8947 - val_loss: 0.5514 - val_accuracy: 0.6842\n",
      "Epoch 89/200\n",
      "760/760 [==============================] - 0s 71us/step - loss: 0.3965 - accuracy: 0.8961 - val_loss: 0.5424 - val_accuracy: 0.7000\n",
      "Epoch 90/200\n",
      "760/760 [==============================] - 0s 71us/step - loss: 0.3916 - accuracy: 0.8987 - val_loss: 0.5381 - val_accuracy: 0.7263\n",
      "Epoch 91/200\n",
      "760/760 [==============================] - 0s 87us/step - loss: 0.3865 - accuracy: 0.9066 - val_loss: 0.5360 - val_accuracy: 0.7316\n",
      "Epoch 92/200\n",
      "760/760 [==============================] - 0s 79us/step - loss: 0.3812 - accuracy: 0.9105 - val_loss: 0.5408 - val_accuracy: 0.6789\n",
      "Epoch 93/200\n",
      "760/760 [==============================] - 0s 82us/step - loss: 0.3767 - accuracy: 0.9092 - val_loss: 0.5348 - val_accuracy: 0.7105\n",
      "Epoch 94/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.3718 - accuracy: 0.9105 - val_loss: 0.5364 - val_accuracy: 0.6947\n",
      "Epoch 95/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.3672 - accuracy: 0.9118 - val_loss: 0.5316 - val_accuracy: 0.7158\n",
      "Epoch 96/200\n",
      "760/760 [==============================] - 0s 99us/step - loss: 0.3626 - accuracy: 0.9092 - val_loss: 0.5314 - val_accuracy: 0.7105\n",
      "Epoch 97/200\n",
      "760/760 [==============================] - 0s 79us/step - loss: 0.3580 - accuracy: 0.9132 - val_loss: 0.5251 - val_accuracy: 0.7316\n",
      "Epoch 98/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.3530 - accuracy: 0.9224 - val_loss: 0.5269 - val_accuracy: 0.7105\n",
      "Epoch 99/200\n",
      "760/760 [==============================] - 0s 95us/step - loss: 0.3488 - accuracy: 0.9158 - val_loss: 0.5230 - val_accuracy: 0.7316\n",
      "Epoch 100/200\n",
      "760/760 [==============================] - 0s 80us/step - loss: 0.3441 - accuracy: 0.9224 - val_loss: 0.5208 - val_accuracy: 0.7421\n",
      "Epoch 101/200\n",
      "760/760 [==============================] - 0s 79us/step - loss: 0.3400 - accuracy: 0.9211 - val_loss: 0.5211 - val_accuracy: 0.7368\n",
      "Epoch 102/200\n",
      "760/760 [==============================] - 0s 91us/step - loss: 0.3359 - accuracy: 0.9224 - val_loss: 0.5205 - val_accuracy: 0.7316\n",
      "Epoch 103/200\n",
      "760/760 [==============================] - 0s 103us/step - loss: 0.3313 - accuracy: 0.9303 - val_loss: 0.5184 - val_accuracy: 0.7421\n",
      "Epoch 104/200\n",
      "760/760 [==============================] - 0s 105us/step - loss: 0.3261 - accuracy: 0.9368 - val_loss: 0.5182 - val_accuracy: 0.7158\n",
      "Epoch 105/200\n",
      "760/760 [==============================] - 0s 92us/step - loss: 0.3225 - accuracy: 0.9368 - val_loss: 0.5200 - val_accuracy: 0.7158\n",
      "Epoch 106/200\n",
      "760/760 [==============================] - 0s 96us/step - loss: 0.3179 - accuracy: 0.9329 - val_loss: 0.5135 - val_accuracy: 0.7316\n",
      "Epoch 107/200\n",
      "760/760 [==============================] - 0s 103us/step - loss: 0.3147 - accuracy: 0.9408 - val_loss: 0.5142 - val_accuracy: 0.7316\n",
      "Epoch 108/200\n",
      "760/760 [==============================] - 0s 88us/step - loss: 0.3098 - accuracy: 0.9434 - val_loss: 0.5134 - val_accuracy: 0.7368\n",
      "Epoch 109/200\n",
      "760/760 [==============================] - 0s 89us/step - loss: 0.3070 - accuracy: 0.9368 - val_loss: 0.5111 - val_accuracy: 0.7263\n",
      "Epoch 110/200\n",
      "760/760 [==============================] - 0s 101us/step - loss: 0.3023 - accuracy: 0.9421 - val_loss: 0.5119 - val_accuracy: 0.7316\n",
      "Epoch 111/200\n",
      "760/760 [==============================] - 0s 91us/step - loss: 0.2982 - accuracy: 0.9500 - val_loss: 0.5088 - val_accuracy: 0.7263\n",
      "Epoch 112/200\n",
      "760/760 [==============================] - 0s 113us/step - loss: 0.2941 - accuracy: 0.9474 - val_loss: 0.5082 - val_accuracy: 0.7263\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/760 [==============================] - 0s 114us/step - loss: 0.2900 - accuracy: 0.9474 - val_loss: 0.5061 - val_accuracy: 0.7421\n",
      "Epoch 114/200\n",
      "760/760 [==============================] - 0s 100us/step - loss: 0.2872 - accuracy: 0.9500 - val_loss: 0.5074 - val_accuracy: 0.7316\n",
      "Epoch 115/200\n",
      "760/760 [==============================] - 0s 88us/step - loss: 0.2830 - accuracy: 0.9500 - val_loss: 0.5069 - val_accuracy: 0.7316\n",
      "Epoch 116/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.2797 - accuracy: 0.9539 - val_loss: 0.5060 - val_accuracy: 0.7316\n",
      "Epoch 117/200\n",
      "760/760 [==============================] - 0s 71us/step - loss: 0.2749 - accuracy: 0.9579 - val_loss: 0.5031 - val_accuracy: 0.7474\n",
      "Epoch 118/200\n",
      "760/760 [==============================] - 0s 67us/step - loss: 0.2714 - accuracy: 0.9592 - val_loss: 0.5037 - val_accuracy: 0.7316\n",
      "Epoch 119/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.2684 - accuracy: 0.9539 - val_loss: 0.5040 - val_accuracy: 0.7316\n",
      "Epoch 120/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.2645 - accuracy: 0.9592 - val_loss: 0.5028 - val_accuracy: 0.7316\n",
      "Epoch 121/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.2607 - accuracy: 0.9618 - val_loss: 0.5048 - val_accuracy: 0.7368\n",
      "Epoch 122/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.2579 - accuracy: 0.9605 - val_loss: 0.5024 - val_accuracy: 0.7316\n",
      "Epoch 123/200\n",
      "760/760 [==============================] - 0s 71us/step - loss: 0.2539 - accuracy: 0.9632 - val_loss: 0.5024 - val_accuracy: 0.7368\n",
      "Epoch 124/200\n",
      "760/760 [==============================] - 0s 80us/step - loss: 0.2513 - accuracy: 0.9645 - val_loss: 0.4996 - val_accuracy: 0.7368\n",
      "Epoch 125/200\n",
      "760/760 [==============================] - 0s 80us/step - loss: 0.2482 - accuracy: 0.9684 - val_loss: 0.5005 - val_accuracy: 0.7368\n",
      "Epoch 126/200\n",
      "760/760 [==============================] - 0s 79us/step - loss: 0.2432 - accuracy: 0.9671 - val_loss: 0.5034 - val_accuracy: 0.7368\n",
      "Epoch 127/200\n",
      "760/760 [==============================] - 0s 97us/step - loss: 0.2400 - accuracy: 0.9632 - val_loss: 0.4988 - val_accuracy: 0.7421\n",
      "Epoch 128/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.2384 - accuracy: 0.9632 - val_loss: 0.4988 - val_accuracy: 0.7421\n",
      "Epoch 129/200\n",
      "760/760 [==============================] - 0s 88us/step - loss: 0.2353 - accuracy: 0.9658 - val_loss: 0.5031 - val_accuracy: 0.7421\n",
      "Epoch 130/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.2313 - accuracy: 0.9671 - val_loss: 0.5010 - val_accuracy: 0.7421\n",
      "Epoch 131/200\n",
      "760/760 [==============================] - 0s 89us/step - loss: 0.2285 - accuracy: 0.9684 - val_loss: 0.5007 - val_accuracy: 0.7421\n",
      "Epoch 132/200\n",
      "760/760 [==============================] - 0s 80us/step - loss: 0.2257 - accuracy: 0.9711 - val_loss: 0.4979 - val_accuracy: 0.7421\n",
      "Epoch 133/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.2225 - accuracy: 0.9697 - val_loss: 0.4999 - val_accuracy: 0.7421\n",
      "Epoch 134/200\n",
      "760/760 [==============================] - 0s 88us/step - loss: 0.2197 - accuracy: 0.9724 - val_loss: 0.5016 - val_accuracy: 0.7474\n",
      "Epoch 135/200\n",
      "760/760 [==============================] - 0s 79us/step - loss: 0.2169 - accuracy: 0.9750 - val_loss: 0.5003 - val_accuracy: 0.7474\n",
      "Epoch 136/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.2134 - accuracy: 0.9737 - val_loss: 0.4993 - val_accuracy: 0.7421\n",
      "Epoch 137/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.2112 - accuracy: 0.9750 - val_loss: 0.5022 - val_accuracy: 0.7474\n",
      "Epoch 138/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.2088 - accuracy: 0.9750 - val_loss: 0.4977 - val_accuracy: 0.7474\n",
      "Epoch 139/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.2054 - accuracy: 0.9750 - val_loss: 0.4958 - val_accuracy: 0.7316\n",
      "Epoch 140/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.2029 - accuracy: 0.9789 - val_loss: 0.4999 - val_accuracy: 0.7474\n",
      "Epoch 141/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.2004 - accuracy: 0.9763 - val_loss: 0.4987 - val_accuracy: 0.7474\n",
      "Epoch 142/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.1971 - accuracy: 0.9789 - val_loss: 0.5028 - val_accuracy: 0.7421\n",
      "Epoch 143/200\n",
      "760/760 [==============================] - 0s 95us/step - loss: 0.1950 - accuracy: 0.9737 - val_loss: 0.4955 - val_accuracy: 0.7368\n",
      "Epoch 144/200\n",
      "760/760 [==============================] - 0s 89us/step - loss: 0.1930 - accuracy: 0.9776 - val_loss: 0.4973 - val_accuracy: 0.7421\n",
      "Epoch 145/200\n",
      "760/760 [==============================] - 0s 91us/step - loss: 0.1889 - accuracy: 0.9816 - val_loss: 0.4961 - val_accuracy: 0.7368\n",
      "Epoch 146/200\n",
      "760/760 [==============================] - 0s 79us/step - loss: 0.1871 - accuracy: 0.9776 - val_loss: 0.4965 - val_accuracy: 0.7474\n",
      "Epoch 147/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.1849 - accuracy: 0.9816 - val_loss: 0.5000 - val_accuracy: 0.7474\n",
      "Epoch 148/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.1818 - accuracy: 0.9803 - val_loss: 0.4995 - val_accuracy: 0.7579\n",
      "Epoch 149/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.1798 - accuracy: 0.9803 - val_loss: 0.4980 - val_accuracy: 0.7474\n",
      "Epoch 150/200\n",
      "760/760 [==============================] - 0s 85us/step - loss: 0.1774 - accuracy: 0.9829 - val_loss: 0.4979 - val_accuracy: 0.7526\n",
      "Epoch 151/200\n",
      "760/760 [==============================] - 0s 91us/step - loss: 0.1748 - accuracy: 0.9803 - val_loss: 0.5026 - val_accuracy: 0.7474\n",
      "Epoch 152/200\n",
      "760/760 [==============================] - 0s 80us/step - loss: 0.1728 - accuracy: 0.9816 - val_loss: 0.4982 - val_accuracy: 0.7526\n",
      "Epoch 153/200\n",
      "760/760 [==============================] - 0s 99us/step - loss: 0.1703 - accuracy: 0.9829 - val_loss: 0.4991 - val_accuracy: 0.7579\n",
      "Epoch 154/200\n",
      "760/760 [==============================] - 0s 89us/step - loss: 0.1679 - accuracy: 0.9842 - val_loss: 0.4976 - val_accuracy: 0.7421\n",
      "Epoch 155/200\n",
      "760/760 [==============================] - 0s 97us/step - loss: 0.1662 - accuracy: 0.9842 - val_loss: 0.4971 - val_accuracy: 0.7474\n",
      "Epoch 156/200\n",
      "760/760 [==============================] - 0s 99us/step - loss: 0.1634 - accuracy: 0.9855 - val_loss: 0.5006 - val_accuracy: 0.7579\n",
      "Epoch 157/200\n",
      "760/760 [==============================] - 0s 96us/step - loss: 0.1616 - accuracy: 0.9855 - val_loss: 0.4992 - val_accuracy: 0.7526\n",
      "Epoch 158/200\n",
      "760/760 [==============================] - 0s 87us/step - loss: 0.1602 - accuracy: 0.9855 - val_loss: 0.5003 - val_accuracy: 0.7579\n",
      "Epoch 159/200\n",
      "760/760 [==============================] - 0s 112us/step - loss: 0.1573 - accuracy: 0.9895 - val_loss: 0.4980 - val_accuracy: 0.7474\n",
      "Epoch 160/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.1553 - accuracy: 0.9882 - val_loss: 0.4983 - val_accuracy: 0.7474\n",
      "Epoch 161/200\n",
      "760/760 [==============================] - 0s 74us/step - loss: 0.1532 - accuracy: 0.9895 - val_loss: 0.5009 - val_accuracy: 0.7579\n",
      "Epoch 162/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.1511 - accuracy: 0.9895 - val_loss: 0.5005 - val_accuracy: 0.7632\n",
      "Epoch 163/200\n",
      "760/760 [==============================] - 0s 70us/step - loss: 0.1494 - accuracy: 0.9882 - val_loss: 0.5013 - val_accuracy: 0.7632\n",
      "Epoch 164/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.1474 - accuracy: 0.9921 - val_loss: 0.5006 - val_accuracy: 0.7579\n",
      "Epoch 165/200\n",
      "760/760 [==============================] - 0s 76us/step - loss: 0.1457 - accuracy: 0.9882 - val_loss: 0.5022 - val_accuracy: 0.7632\n",
      "Epoch 166/200\n",
      "760/760 [==============================] - 0s 82us/step - loss: 0.1436 - accuracy: 0.9908 - val_loss: 0.5029 - val_accuracy: 0.7632\n",
      "Epoch 167/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.1418 - accuracy: 0.9921 - val_loss: 0.5034 - val_accuracy: 0.7632\n",
      "Epoch 168/200\n",
      "760/760 [==============================] - 0s 78us/step - loss: 0.1401 - accuracy: 0.9908 - val_loss: 0.5025 - val_accuracy: 0.7632\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/760 [==============================] - 0s 103us/step - loss: 0.1390 - accuracy: 0.9934 - val_loss: 0.5031 - val_accuracy: 0.7632\n",
      "Epoch 170/200\n",
      "760/760 [==============================] - 0s 95us/step - loss: 0.1364 - accuracy: 0.9947 - val_loss: 0.5016 - val_accuracy: 0.7579\n",
      "Epoch 171/200\n",
      "760/760 [==============================] - 0s 72us/step - loss: 0.1349 - accuracy: 0.9947 - val_loss: 0.5020 - val_accuracy: 0.7526\n",
      "Epoch 172/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.1323 - accuracy: 0.9934 - val_loss: 0.5069 - val_accuracy: 0.7579\n",
      "Epoch 173/200\n",
      "760/760 [==============================] - 0s 75us/step - loss: 0.1313 - accuracy: 0.9947 - val_loss: 0.5039 - val_accuracy: 0.7579\n",
      "Epoch 174/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.1298 - accuracy: 0.9947 - val_loss: 0.5066 - val_accuracy: 0.7632\n",
      "Epoch 175/200\n",
      "760/760 [==============================] - 0s 107us/step - loss: 0.1277 - accuracy: 0.9947 - val_loss: 0.5079 - val_accuracy: 0.7579\n",
      "Epoch 176/200\n",
      "760/760 [==============================] - 0s 103us/step - loss: 0.1263 - accuracy: 0.9961 - val_loss: 0.5045 - val_accuracy: 0.7579\n",
      "Epoch 177/200\n",
      "760/760 [==============================] - 0s 93us/step - loss: 0.1249 - accuracy: 0.9987 - val_loss: 0.5043 - val_accuracy: 0.7526\n",
      "Epoch 178/200\n",
      "760/760 [==============================] - 0s 103us/step - loss: 0.1228 - accuracy: 0.9987 - val_loss: 0.5099 - val_accuracy: 0.7632\n",
      "Epoch 179/200\n",
      "760/760 [==============================] - 0s 96us/step - loss: 0.1218 - accuracy: 0.9974 - val_loss: 0.5092 - val_accuracy: 0.7632\n",
      "Epoch 180/200\n",
      "760/760 [==============================] - 0s 107us/step - loss: 0.1203 - accuracy: 0.9974 - val_loss: 0.5071 - val_accuracy: 0.7579\n",
      "Epoch 181/200\n",
      "760/760 [==============================] - 0s 103us/step - loss: 0.1182 - accuracy: 0.9987 - val_loss: 0.5093 - val_accuracy: 0.7632\n",
      "Epoch 182/200\n",
      "760/760 [==============================] - 0s 110us/step - loss: 0.1174 - accuracy: 0.9987 - val_loss: 0.5074 - val_accuracy: 0.7579\n",
      "Epoch 183/200\n",
      "760/760 [==============================] - 0s 105us/step - loss: 0.1157 - accuracy: 0.9987 - val_loss: 0.5080 - val_accuracy: 0.7579\n",
      "Epoch 184/200\n",
      "760/760 [==============================] - 0s 112us/step - loss: 0.1134 - accuracy: 0.9987 - val_loss: 0.5056 - val_accuracy: 0.7421\n",
      "Epoch 185/200\n",
      "760/760 [==============================] - 0s 104us/step - loss: 0.1135 - accuracy: 0.9987 - val_loss: 0.5097 - val_accuracy: 0.7579\n",
      "Epoch 186/200\n",
      "760/760 [==============================] - 0s 101us/step - loss: 0.1114 - accuracy: 0.9987 - val_loss: 0.5089 - val_accuracy: 0.7579\n",
      "Epoch 187/200\n",
      "760/760 [==============================] - 0s 100us/step - loss: 0.1100 - accuracy: 0.9987 - val_loss: 0.5099 - val_accuracy: 0.7526\n",
      "Epoch 188/200\n",
      "760/760 [==============================] - 0s 96us/step - loss: 0.1091 - accuracy: 0.9987 - val_loss: 0.5110 - val_accuracy: 0.7526\n",
      "Epoch 189/200\n",
      "760/760 [==============================] - 0s 88us/step - loss: 0.1076 - accuracy: 0.9987 - val_loss: 0.5154 - val_accuracy: 0.7684\n",
      "Epoch 190/200\n",
      "760/760 [==============================] - 0s 87us/step - loss: 0.1059 - accuracy: 0.9987 - val_loss: 0.5126 - val_accuracy: 0.7632\n",
      "Epoch 191/200\n",
      "760/760 [==============================] - 0s 85us/step - loss: 0.1047 - accuracy: 0.9987 - val_loss: 0.5103 - val_accuracy: 0.7526\n",
      "Epoch 192/200\n",
      "760/760 [==============================] - 0s 96us/step - loss: 0.1038 - accuracy: 0.9987 - val_loss: 0.5138 - val_accuracy: 0.7632\n",
      "Epoch 193/200\n",
      "760/760 [==============================] - 0s 101us/step - loss: 0.1023 - accuracy: 0.9987 - val_loss: 0.5118 - val_accuracy: 0.7526\n",
      "Epoch 194/200\n",
      "760/760 [==============================] - 0s 105us/step - loss: 0.1016 - accuracy: 0.9987 - val_loss: 0.5129 - val_accuracy: 0.7579\n",
      "Epoch 195/200\n",
      "760/760 [==============================] - 0s 99us/step - loss: 0.0999 - accuracy: 0.9987 - val_loss: 0.5125 - val_accuracy: 0.7474\n",
      "Epoch 196/200\n",
      "760/760 [==============================] - 0s 99us/step - loss: 0.0988 - accuracy: 0.9987 - val_loss: 0.5123 - val_accuracy: 0.7474\n",
      "Epoch 197/200\n",
      "760/760 [==============================] - 0s 126us/step - loss: 0.0975 - accuracy: 0.9987 - val_loss: 0.5142 - val_accuracy: 0.7526\n",
      "Epoch 198/200\n",
      "760/760 [==============================] - 0s 97us/step - loss: 0.0964 - accuracy: 0.9987 - val_loss: 0.5165 - val_accuracy: 0.7684\n",
      "Epoch 199/200\n",
      "760/760 [==============================] - 0s 87us/step - loss: 0.0955 - accuracy: 0.9987 - val_loss: 0.5153 - val_accuracy: 0.7526\n",
      "Epoch 200/200\n",
      "760/760 [==============================] - 0s 89us/step - loss: 0.0941 - accuracy: 0.9987 - val_loss: 0.5161 - val_accuracy: 0.7579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25339348630>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train, y_train, epochs = num_epochs, verbose = 1, \n",
    "          validation_split = 0.2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict on both train and test data to evaluate the performance of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y_true, y_pred, title, labels):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    print(f\"Accuracy:  {np.round((tp + tn) / (tp + fp + tn + fn), 3)}\")\n",
    "    print(f\"Recall:    {np.round(tp / (tp + fn), 3)}\")\n",
    "    print(f\"Precision: {np.round(tp / (tp + fp), 3)}\")\n",
    "    print(f\"f1 score:  {np.round(2 * tp / (2 * tp + fp + fn), 3)}\\n\")\n",
    "    \n",
    "    data = confusion_matrix(y_true, y_pred)\n",
    "    data = pd.DataFrame(data/np.sum(data), index = labels, columns = labels)\n",
    "    print(data)\n",
    "    akws = {\"ha\": 'center',\"va\": 'center', 'size':15}\n",
    "    sns.heatmap(data, annot = True, annot_kws=akws)\n",
    "#     plt.axis('Off')\n",
    "    plt.suptitle(title)\n",
    "    plt.xticks(rotation = 90); plt.yticks(rotation = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on Train** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.951\n",
      "Recall:    0.943\n",
      "Precision: 0.957\n",
      "f1 score:  0.95\n",
      "\n",
      "          Negative  Positive\n",
      "Negative  0.478947  0.021053\n",
      "Positive  0.028421  0.471579\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_train = model.predict_classes(padded_train)\n",
    "evaluate_performance(y_train, y_pred_train, title = 'Train Performance',\n",
    "                     labels = ['Negative', 'Positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on Test** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.86\n",
      "Recall:    0.88\n",
      "Precision: 0.846\n",
      "f1 score:  0.863\n",
      "\n",
      "          Negative  Positive\n",
      "Negative  0.42      0.08    \n",
      "Positive  0.06      0.44    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_test = model.predict_classes(padded_test)\n",
    "evaluate_performance(y_test, y_pred_test, title = 'Test Performance',\n",
    "                     labels = ['Negative', 'Positive'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
